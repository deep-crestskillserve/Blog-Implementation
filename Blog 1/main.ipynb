{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [Blog 1: Build generative AI applications on Amazon Bedrock with the AWS SDK for Python (Boto3)](https://aws.amazon.com/blogs/machine-learning/build-generative-ai-applications-on-amazon-bedrock-with-the-aws-sdk-for-python-boto3/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Import the required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "RdD21SzfvrNR"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dQ9nbgcNvvoG"
      },
      "outputs": [],
      "source": [
        "# Explicitly define AWS credentials in the session\n",
        "# Follow this step when running this on Google collab\n",
        "session = boto3.Session(\n",
        "    aws_access_key_id=\"AKIAQXUIXLSSGCYCWMLH\",\n",
        "    aws_secret_access_key=\"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\",\n",
        "    region_name=\"us-east-1\"\n",
        ")\n",
        "# Set up the Amazon Bedrock client\n",
        "bedrock_client = session.client(\"bedrock-runtime\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Set up the Boto3 client to use the Amazon Bedrock runtime and specify the AWS Region:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up the Amazon Bedrock client\n",
        "bedrock_client = boto3.client(\n",
        "    service_name=\"bedrock-runtime\",\n",
        "    region_name=\"us-east-1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Define the model to invoke using its model ID. In this example, we use Anthropic‚Äôs Claude 3 Sonnet on Amazon Bedrock:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DAE8SMKZxmcv"
      },
      "outputs": [],
      "source": [
        "# Define the model ID\n",
        "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Assign a prompt, which is your message that will be used to interact with the FM at invocation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fcyt-74AxqMS"
      },
      "outputs": [],
      "source": [
        "# Prepare the input prompt\n",
        "prompt = \"Hello, how are you?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. Define the payload as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bfvbYYwLxssx"
      },
      "outputs": [],
      "source": [
        "# Create the request payload\n",
        "payload = {\n",
        "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "    \"max_tokens\": 2048,\n",
        "    \"temperature\": 0.9,\n",
        "    \"top_k\": 250,\n",
        "    \"top_p\": 1,\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": prompt\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6. You have set the parameters and the FM you want to interact with. Now you send a request to Amazon Bedrock by providing the FM to interact with and the payload that you defined:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ISU-JA3Vxuap"
      },
      "outputs": [],
      "source": [
        "# Invoke the Amazon Bedrock model\n",
        "response = bedrock_client.invoke_model(\n",
        "    modelId=model_id,\n",
        "    body=json.dumps(payload)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7. After the request is processed, you can display the result of the generated text from Amazon Bedrock:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ResponseMetadata': {'RequestId': 'c5336bc4-9404-47e7-9183-bdafbade4dec', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 28 Mar 2025 05:22:18 GMT', 'content-type': 'application/json', 'content-length': '429', 'connection': 'keep-alive', 'x-amzn-requestid': 'c5336bc4-9404-47e7-9183-bdafbade4dec', 'x-amzn-bedrock-invocation-latency': '1127', 'x-amzn-bedrock-output-token-count': '45', 'x-amzn-bedrock-input-token-count': '13'}, 'RetryAttempts': 0}, 'contentType': 'application/json', 'body': <botocore.response.StreamingBody object at 0x000001C4816DC760>}\n",
            "\n",
            "[{'type': 'text', 'text': \"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\"}]\n",
            "\n",
            "Response: Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "print(response)\n",
        "\n",
        "print()\n",
        "\n",
        "result = json.loads(response[\"body\"].read())\n",
        "\n",
        "# Ensure 'content' exists and is a list\n",
        "if \"content\" in result and isinstance(result[\"content\"], list):\n",
        "    print(result[\"content\"])  # Print the content list\n",
        "\n",
        "    # c = result[\"content\"]\n",
        "    # print(c[\"text\"])\n",
        "    # Extract the text from each content dictionary in the list\n",
        "    generated_text = \"\".join(output[\"text\"] for output in result[\"content\"] if \"text\" in output)\n",
        "\n",
        "    print(f\"\\nResponse: {generated_text}\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: No valid 'content' found in the response.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Djux0P5pxwIm",
        "outputId": "a6430ebc-18ea-494d-e3b8-bcb01bf65aa9"
      },
      "outputs": [
        {
          "ename": "UnsupportedOperation",
          "evalue": "seek",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[47], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m response_body \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Reset the stream before reading\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mresponse_body\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Read response content (convert to string immediately)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     body_content \u001b[38;5;241m=\u001b[39m response_body\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(response_body, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m response_body\n",
            "\u001b[1;31mUnsupportedOperation\u001b[0m: seek"
          ]
        }
      ],
      "source": [
        "# print(response)\n",
        "# print()\n",
        "\n",
        "if response and \"body\" in response and response[\"body\"]:\n",
        "    response_body = response[\"body\"]\n",
        "\n",
        "    try:\n",
        "        # Reset the stream before reading\n",
        "        response_body.seek(0)\n",
        "\n",
        "        # Read response content (convert to string immediately)\n",
        "        body_content = response_body.read().decode(\"utf-8\") if hasattr(response_body, \"read\") else response_body\n",
        "\n",
        "        if body_content:  # Check if the response body is not empty\n",
        "            # Parse JSON response\n",
        "            result = json.loads(body_content)\n",
        "            \n",
        "            # Extract content safely\n",
        "            content = result.get(\"content\", [])\n",
        "            print(content)\n",
        "            print()\n",
        "            \n",
        "            # Generate final text output safely\n",
        "            generated_text = \"\".join(output.get(\"text\", \"\") for output in content)\n",
        "            print(f\"Response: {generated_text}\")\n",
        "\n",
        "            # Reset the stream again in case it's needed for further operations\n",
        "            response_body.seek(0)\n",
        "        \n",
        "        else:\n",
        "            print(\"Error: Response body is empty.\")\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error: Failed to decode JSON response. Raw response: {body_content}\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: No response body found.\")\n",
        "\n",
        "# response_body.seek(0)\n",
        "# print(result[\"content\"])\n",
        "# print()\n",
        "# generated_text = \"\".join([output[\"text\"] for output in result[\"content\"]])\n",
        "# print(f\"Response: {generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Steps to Invoke Amazon Bedrock Model\n",
        "\n",
        "## 1. Import Necessary Libraries\n",
        "Import required Python libraries to work with AWS Bedrock and JSON.\n",
        "\n",
        "## 2. Set Up Boto3 for Accessing Service\n",
        "- Configure `boto3` to access the Bedrock runtime.\n",
        "- Define the AWS region based on the intended user to ensure low latency.\n",
        "- If no `region_name` is defined, it will use the default region set in the AWS CLI.\n",
        "\n",
        "## 3. Define the Model ID\n",
        "- Choose the model to invoke based on the use case.\n",
        "- Example: `anthropic.claude-v2`.\n",
        "\n",
        "## 4. Prepare the Input Prompt\n",
        "- Define the user query that will be sent to the model.\n",
        "\n",
        "## 5. Assign the Prompt into the Payload\n",
        "The payload consists of:\n",
        "- **`anthropic_version`**: Specifies the Bedrock API version for Anthropic models.\n",
        "- **`max_tokens`**: Limits the maximum number of tokens a model can generate.\n",
        "- **`temperature`**: Controls the randomness of the model.\n",
        "- **`top_k`**: Defines how many of the top-ranked words the model considers for generating a response.\n",
        "- **`top_p`**: Controls how diverse the word choices are.\n",
        "- **`messages`**: Contains the conversation history.\n",
        "  - `{ \"role\": \"user\" }`: Specifies the speaker.\n",
        "    - `user`: The person interacting with the model.\n",
        "    - `assistant`: The AI's response.\n",
        "    - `system`: Special instructions on how the AI should behave.\n",
        "  - **`content`**: The actual message.\n",
        "    - `type`: \"text\" (specifies plain text input).\n",
        "    - `text`: The actual user input text.\n",
        "\n",
        "## 6. Invoke the Amazon Bedrock Model\n",
        "- Define the `model_id` and `body` to send to the Bedrock model.\n",
        "- Convert the Python dictionary payload into a JSON string using `json.dumps(payload)`.\n",
        "\n",
        "## 7. Process the Response\n",
        "- The response from the model invocation is stored in the variable `response`.\n",
        "- The result is obtained using:\n",
        "  ```python\n",
        "  result = json.loads(response[\"body\"].read())\n",
        "  ```\n",
        "- The response from the Claude Anthropic model is returned in JSON format.\n",
        "\n",
        "### Response Structure\n",
        "The response consists of a JSON object with the following structure:\n",
        "\n",
        "- **`ResponseMetadata`**\n",
        "  - `RequestId`\n",
        "  - `HTTPStatusCode`\n",
        "  - `HTTPHeaders`\n",
        "    - `date`\n",
        "    - `content-type`\n",
        "    - `content-length`\n",
        "    - `connection`\n",
        "    - `x-amzn-requestid`\n",
        "    - `x-amzn-bedrock-invocation-latency`\n",
        "    - `x-amzn-bedrock-output-token-count`\n",
        "    - `x-amzn-bedrock-input-token-count`\n",
        "  - `RetryAttempts`\n",
        "- **`contentType`**\n",
        "- **`body`** (Wrapped inside a `StreamingBody` object)\n",
        "\n",
        "### Handling the StreamingBody\n",
        "- Everything except `body` is stored in a human-readable format.\n",
        "- The `body` is wrapped inside a `StreamingBody` object, which is a special streaming data wrapper provided by Boto3.\n",
        "- Instead of returning a normal JSON string, AWS Bedrock streams the response in chunks for optimized performance, especially for large responses.\n",
        "\n",
        "#### Comparison of Streaming Methods\n",
        "| Feature                | Boto3 (StreamingBody) | Requests (HTTP) | OpenAI API (GPT) | FastAPI (Web Apps) |\n",
        "|------------------------|----------------------|----------------|------------------|---------------------|\n",
        "| Streaming Object      | ‚úÖ StreamingBody     | ‚úÖ Response.raw | ‚úÖ Streamed JSON | ‚úÖ StreamingResponse |\n",
        "| AWS-Specific?         | ‚úÖ Yes               | ‚ùå No          | ‚ùå No            | ‚ùå No               |\n",
        "| Chunk Processing      | `.read()`, `.iter_chunks()` | `.iter_content()` | `.get(\"content\")` | `yield` |\n",
        "| Used For              | S3, Bedrock, Textract | Large HTTP responses | AI Model Responses | Large Web Responses |\n",
        "\n",
        "### Key Takeaways:\n",
        "- `StreamingBody` does not store the response as a string but as a byte stream.\n",
        "- Calling `.read()` retrieves all bytes from the stream at once.\n",
        "- To process large responses efficiently, use `.iter_chunks()` instead of `.read()`.\n",
        "\n",
        "---\n",
        "\n",
        "This document provides a structured step-by-step guide for invoking Amazon Bedrock models efficiently.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# final combined code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! As an AI language model, I don't have feelings, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "#1. import libraries\n",
        "import boto3\n",
        "import json\n",
        "\n",
        "#2. make bedrock_client\n",
        "bedrock_client = boto3.client(\n",
        "    service_name = \"bedrock-runtime\",\n",
        "    region_name = \"us-east-1\"\n",
        ")\n",
        "\n",
        "#3. define the model id\n",
        "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
        "\n",
        "#4. prompt\n",
        "prompt = \"Hello, how are you?\"\n",
        "\n",
        "#5. payload\n",
        "payload = {\n",
        "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "    \"max_tokens\": 2048,\n",
        "    \"top_p\": 0.9,\n",
        "    \"top_k\": 250,\n",
        "    \"temperature\": 0.9,\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": prompt\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "response = bedrock_client.invoke_model (\n",
        "    modelId = model_id,\n",
        "    body = json.dumps(payload)\n",
        ")\n",
        "\n",
        "result = json.loads(response[\"body\"].read())\n",
        "generated_text = \"\".join([output[\"text\"] for output in result[\"content\"]])\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'type': 'text', 'text': \"Sure, here's a joke for you:\\n\\nWhy can't a bicycle stand up by itself? Because it's two-tired!\"}]\n",
            "\n",
            "Response: Sure, here's a joke for you:\n",
            "\n",
            "Why can't a bicycle stand up by itself? Because it's two-tired!\n"
          ]
        }
      ],
      "source": [
        "import boto3\n",
        "import json\n",
        "\n",
        "# Set up the Amazon Bedrock client\n",
        "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
        "\n",
        "# Define the model ID\n",
        "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
        "\n",
        "# Prepare the input payload with alternating user and assistant messages\n",
        "payload = {\n",
        "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "    \"max_tokens\": 2048,\n",
        "    \"temperature\": 0.9,\n",
        "    \"top_k\": 250,\n",
        "    \"top_p\": 1,\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"Hello, how are you?\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"I'm doing great! How can I assist you today?\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"What is the capital of France?\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"The capital of France is Paris.\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"Can you tell me a joke?\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Invoke the model\n",
        "response = bedrock_client.invoke_model(\n",
        "    modelId=model_id,\n",
        "    body=json.dumps(payload)\n",
        ")\n",
        "\n",
        "# Process the response\n",
        "result = json.loads(response[\"body\"].read())\n",
        "\n",
        "# Ensure 'content' exists and is a list\n",
        "if \"content\" in result and isinstance(result[\"content\"], list):\n",
        "    print(result[\"content\"])  # Print the content list\n",
        "\n",
        "    # Extract all text elements\n",
        "    generated_text = \"\".join(output[\"text\"] for output in result[\"content\"] if \"text\" in output)\n",
        "    print(f\"\\nResponse: {generated_text}\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: No valid 'content' found in the response.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîπ Approach 1: Sequential API Calls for Each User Message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello, how are you?\n",
            "Assistant: Hello! As an AI language model, I don't have feelings, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\n",
            "\n",
            "User: What is the capital of France?\n",
            "Assistant: The capital of France is Paris.\n",
            "\n",
            "User: Can you tell me a joke?\n",
            "Assistant: Sure, here's a joke for you:\n",
            "\n",
            "Why did the scarecrow win an award? Because he was outstanding in his field!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import boto3\n",
        "import json\n",
        "\n",
        "# Set up the Amazon Bedrock client\n",
        "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
        "\n",
        "# Define the model ID\n",
        "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
        "\n",
        "# List of user messages to send one by one\n",
        "user_messages = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"What is the capital of France?\",\n",
        "    \"Can you tell me a joke?\"\n",
        "]\n",
        "\n",
        "# Initialize conversation history\n",
        "conversation = []\n",
        "\n",
        "# Iterate through user messages and get a response for each\n",
        "for message in user_messages:\n",
        "    # Add the user message to the conversation\n",
        "    conversation.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": message}]})\n",
        "\n",
        "    # Construct the payload for this request\n",
        "    payload = {\n",
        "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "        \"max_tokens\": 256,\n",
        "        \"temperature\": 0.9,\n",
        "        \"top_k\": 250,\n",
        "        \"top_p\": 1,\n",
        "        \"messages\": conversation  # Send the full conversation history\n",
        "    }\n",
        "\n",
        "    # Invoke the model\n",
        "    response = bedrock_client.invoke_model(\n",
        "        modelId=model_id,\n",
        "        body=json.dumps(payload)\n",
        "    )\n",
        "\n",
        "    # Process the response\n",
        "    result = json.loads(response[\"body\"].read())\n",
        "\n",
        "    # Extract the assistant's response\n",
        "    if \"content\" in result and isinstance(result[\"content\"], list):\n",
        "        assistant_response = result[\"content\"][0][\"text\"]\n",
        "        print(f\"User: {message}\")\n",
        "        print(f\"Assistant: {assistant_response}\\n\")\n",
        "\n",
        "        # Add the assistant response to the conversation history\n",
        "        conversation.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant_response}]})\n",
        "\n",
        "    else:\n",
        "        print(\"Error: No valid 'content' found in the response.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
