{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [Build generative AI applications on Amazon Bedrock with the AWS SDK for Python (Boto3)](https://aws.amazon.com/blogs/machine-learning/build-generative-ai-applications-on-amazon-bedrock-with-the-aws-sdk-for-python-boto3/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Import the required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "RdD21SzfvrNR"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dQ9nbgcNvvoG"
      },
      "outputs": [],
      "source": [
        "# Explicitly define AWS credentials in the session\n",
        "# Follow this step when running this on Google collab\n",
        "session = boto3.Session(\n",
        "    aws_access_key_id=\"AKIAQXUIXLSSGCYCWMLH\",\n",
        "    aws_secret_access_key=\"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\",\n",
        "    region_name=\"us-east-1\"\n",
        ")\n",
        "# Set up the Amazon Bedrock client\n",
        "bedrock_client = session.client(\"bedrock-runtime\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Set up the Boto3 client to use the Amazon Bedrock runtime and specify the AWS Region:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up the Amazon Bedrock client\n",
        "bedrock_client = boto3.client(\n",
        "    service_name=\"bedrock-runtime\",\n",
        "    region_name=\"us-east-1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Define the model to invoke using its model ID. In this example, we use Anthropic’s Claude 3 Sonnet on Amazon Bedrock:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DAE8SMKZxmcv"
      },
      "outputs": [],
      "source": [
        "# Define the model ID\n",
        "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Assign a prompt, which is your message that will be used to interact with the FM at invocation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fcyt-74AxqMS"
      },
      "outputs": [],
      "source": [
        "# Prepare the input prompt\n",
        "prompt = \"Hello, how are you?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. Define the payload as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bfvbYYwLxssx"
      },
      "outputs": [],
      "source": [
        "# Create the request payload\n",
        "payload = {\n",
        "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "    \"max_tokens\": 2048,\n",
        "    \"temperature\": 0.9,\n",
        "    \"top_k\": 250,\n",
        "    \"top_p\": 1,\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": prompt\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6. You have set the parameters and the FM you want to interact with. Now you send a request to Amazon Bedrock by providing the FM to interact with and the payload that you defined:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ISU-JA3Vxuap"
      },
      "outputs": [],
      "source": [
        "# Invoke the Amazon Bedrock model\n",
        "response = bedrock_client.invoke_model(\n",
        "    modelId=model_id,\n",
        "    body=json.dumps(payload)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7. After the request is processed, you can display the result of the generated text from Amazon Bedrock:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ResponseMetadata': {'RequestId': 'c5336bc4-9404-47e7-9183-bdafbade4dec', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 28 Mar 2025 05:22:18 GMT', 'content-type': 'application/json', 'content-length': '429', 'connection': 'keep-alive', 'x-amzn-requestid': 'c5336bc4-9404-47e7-9183-bdafbade4dec', 'x-amzn-bedrock-invocation-latency': '1127', 'x-amzn-bedrock-output-token-count': '45', 'x-amzn-bedrock-input-token-count': '13'}, 'RetryAttempts': 0}, 'contentType': 'application/json', 'body': <botocore.response.StreamingBody object at 0x000001C4816DC760>}\n",
            "\n",
            "[{'type': 'text', 'text': \"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\"}]\n",
            "\n",
            "Response: Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "print(response)\n",
        "\n",
        "print()\n",
        "\n",
        "result = json.loads(response[\"body\"].read())\n",
        "\n",
        "# Ensure 'content' exists and is a list\n",
        "if \"content\" in result and isinstance(result[\"content\"], list):\n",
        "    print(result[\"content\"])  # Print the content list\n",
        "\n",
        "    # c = result[\"content\"]\n",
        "    # print(c[\"text\"])\n",
        "    # Extract the text from each content dictionary in the list\n",
        "    generated_text = \"\".join(output[\"text\"] for output in result[\"content\"] if \"text\" in output)\n",
        "\n",
        "    print(f\"\\nResponse: {generated_text}\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: No valid 'content' found in the response.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Djux0P5pxwIm",
        "outputId": "a6430ebc-18ea-494d-e3b8-bcb01bf65aa9"
      },
      "outputs": [
        {
          "ename": "UnsupportedOperation",
          "evalue": "seek",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[47], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m response_body \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Reset the stream before reading\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mresponse_body\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Read response content (convert to string immediately)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     body_content \u001b[38;5;241m=\u001b[39m response_body\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(response_body, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m response_body\n",
            "\u001b[1;31mUnsupportedOperation\u001b[0m: seek"
          ]
        }
      ],
      "source": [
        "# print(response)\n",
        "# print()\n",
        "\n",
        "if response and \"body\" in response and response[\"body\"]:\n",
        "    response_body = response[\"body\"]\n",
        "\n",
        "    try:\n",
        "        # Reset the stream before reading\n",
        "        response_body.seek(0)\n",
        "\n",
        "        # Read response content (convert to string immediately)\n",
        "        body_content = response_body.read().decode(\"utf-8\") if hasattr(response_body, \"read\") else response_body\n",
        "\n",
        "        if body_content:  # Check if the response body is not empty\n",
        "            # Parse JSON response\n",
        "            result = json.loads(body_content)\n",
        "            \n",
        "            # Extract content safely\n",
        "            content = result.get(\"content\", [])\n",
        "            print(content)\n",
        "            print()\n",
        "            \n",
        "            # Generate final text output safely\n",
        "            generated_text = \"\".join(output.get(\"text\", \"\") for output in content)\n",
        "            print(f\"Response: {generated_text}\")\n",
        "\n",
        "            # Reset the stream again in case it's needed for further operations\n",
        "            response_body.seek(0)\n",
        "        \n",
        "        else:\n",
        "            print(\"Error: Response body is empty.\")\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error: Failed to decode JSON response. Raw response: {body_content}\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: No response body found.\")\n",
        "\n",
        "# response_body.seek(0)\n",
        "# print(result[\"content\"])\n",
        "# print()\n",
        "# generated_text = \"\".join([output[\"text\"] for output in result[\"content\"]])\n",
        "# print(f\"Response: {generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "steps\n",
        "(1) import necessary libraries\n",
        "(2) set up boto3 for accessing service, bedrock runtime and define the aws region name based on the intended user to ensure low latency, and if no region_name defined then it will take default region_name defined in the aws cli of the developer\n",
        "(3) define the model id to invoke for the implementation of the use case\n",
        "(4) prepare the input prompt\n",
        "(5) assign a prompt into the payload\n",
        "    payload consists of:\n",
        "    \"anthropic version\": Specifies the Bedrock API version for Anthropic models.\n",
        "    \"max_tokens\": limits the maximum number of token a model can generate\n",
        "    \"temperature\": controls the randomness of the model\n",
        "    \"top_k\": defines how many of the top ranked words the model considers for generating a response\n",
        "    \"top_p\": controls how diverse the word choice are\n",
        "    \"messages\": the list contains the conversation history\n",
        "        {\"role\": \"user\"}: this tells the ai who is speaking\n",
        "            \"user\": the person interacting with the model\n",
        "            \"assistant\": the ai's response\n",
        "            \"sysyem\": special instruction on how the ai should behave\n",
        "        \"content\": the actual message\n",
        "            \"type\": \"text\", this specifies that the input is plain text\n",
        "            \"text\": prompt, the actual user input text\n",
        "(6) invoke the amazon bedrock model\n",
        "    define the model_id and body to be sent to the bedrock model\n",
        "    here, python dictionary payload is converted into the json string using json.dumps(payload)\n",
        "(7) process the response\n",
        "    response from the model invocation is stored inside the variable \"response\"\n",
        "    result is obtained using, result = json.loads(response[\"body\"].read())\n",
        "        reponse comes from the claude anthropic model in a JSON format\n",
        "        the response contains of json object, structure of the same is as followed:\n",
        "            ResponseMetadata\n",
        "                RequestId\n",
        "                HTTPStatusCode\n",
        "                HTTPHeaders\n",
        "                    date\n",
        "                    content-type\n",
        "                    content-length\n",
        "                    connection\n",
        "                    x-amzn-requestid\n",
        "                    x-amzn-bedrock-invocation-latency\n",
        "                    x-amzn-bedrock-output-token-count\n",
        "                    x-amzn-bedrock-input-token-count\n",
        "                RetryAttempts\n",
        "            contentType\n",
        "            body\n",
        "            ResponseMetadata: Contains metadata (status code, headers, etc.).\n",
        "            body: Instead of actual JSON, you get <botocore.response.StreamingBody object>.\n",
        "            💡 This means the response body is not yet readable—it needs to be processed!\n",
        "\n",
        "\n",
        "            all the things except body is stored in a human readable format, However, the response body is not a simple JSON object—instead, it is wrapped inside a StreamingBody object.\n",
        "                streaming body: it is a special streaming data wrapper provided by Boto3\n",
        "                Instead of returning a normal JSON string, AWS Bedrock streams the response in chunks to optimize performance, especially for large responses.\n",
        "                \n",
        "                Summary: StreamingBody vs Other Streaming Methods\n",
        "                You cannot directly print or access StreamingBody like a normal dictionary or string—you must read it first. to do so there are methods like .read() and iter_chunks()\n",
        "                Feature\tboto3       (StreamingBody)\t      requests (HTTP)\tOpenAI API (GPT)\t    FastAPI (Web Apps)\n",
        "                Streaming Object\t✅ StreamingBody\t    ✅ Response.raw\t ✅ Streamed JSON\t   ✅ StreamingResponse\n",
        "                AWS-Specific?\t    ✅ Yes\t            ❌ No\t         ❌ No\t               ❌ No\n",
        "                Chunk Processing\t.read(),              .iter_content()\t.get(\"content\")\t        yield\n",
        "                                    .iter_chunks()\t\n",
        "                Used For\t        S3, Bedrock,          Large HTTP responses\tAI Model Responses\tLarge Web Responses \n",
        "                                    Textract\t\n",
        "\n",
        "            StreamingBody does not store the response as a string but as a byte stream.\n",
        "            Calling .read() retrieves all bytes from the stream at once.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# final combined code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! As an AI language model, I don't have feelings, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "#1. import libraries\n",
        "import boto3\n",
        "import json\n",
        "\n",
        "#2. make bedrock_client\n",
        "bedrock_client = boto3.client(\n",
        "    service_name = \"bedrock-runtime\",\n",
        "    region_name = \"us-east-1\"\n",
        ")\n",
        "\n",
        "#3. define the model id\n",
        "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
        "\n",
        "#4. prompt\n",
        "prompt = \"Hello, how are you?\"\n",
        "\n",
        "#5. payload\n",
        "payload = {\n",
        "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "    \"max_tokens\": 2048,\n",
        "    \"top_p\": 0.9,\n",
        "    \"top_k\": 250,\n",
        "    \"temperature\": 0.9,\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": prompt\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "response = bedrock_client.invoke_model (\n",
        "    modelId = model_id,\n",
        "    body = json.dumps(payload)\n",
        ")\n",
        "\n",
        "result = json.loads(response[\"body\"].read())\n",
        "generated_text = \"\".join([output[\"text\"] for output in result[\"content\"]])\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'type': 'text', 'text': \"Sure, here's a joke for you:\\n\\nWhy can't a bicycle stand up by itself? Because it's two-tired!\"}]\n",
            "\n",
            "Response: Sure, here's a joke for you:\n",
            "\n",
            "Why can't a bicycle stand up by itself? Because it's two-tired!\n"
          ]
        }
      ],
      "source": [
        "import boto3\n",
        "import json\n",
        "\n",
        "# Set up the Amazon Bedrock client\n",
        "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
        "\n",
        "# Define the model ID\n",
        "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
        "\n",
        "# Prepare the input payload with alternating user and assistant messages\n",
        "payload = {\n",
        "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "    \"max_tokens\": 2048,\n",
        "    \"temperature\": 0.9,\n",
        "    \"top_k\": 250,\n",
        "    \"top_p\": 1,\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"Hello, how are you?\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"I'm doing great! How can I assist you today?\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"What is the capital of France?\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"The capital of France is Paris.\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"Can you tell me a joke?\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Invoke the model\n",
        "response = bedrock_client.invoke_model(\n",
        "    modelId=model_id,\n",
        "    body=json.dumps(payload)\n",
        ")\n",
        "\n",
        "# Process the response\n",
        "result = json.loads(response[\"body\"].read())\n",
        "\n",
        "# Ensure 'content' exists and is a list\n",
        "if \"content\" in result and isinstance(result[\"content\"], list):\n",
        "    print(result[\"content\"])  # Print the content list\n",
        "\n",
        "    # Extract all text elements\n",
        "    generated_text = \"\".join(output[\"text\"] for output in result[\"content\"] if \"text\" in output)\n",
        "    print(f\"\\nResponse: {generated_text}\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: No valid 'content' found in the response.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🔹 Approach 1: Sequential API Calls for Each User Message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello, how are you?\n",
            "Assistant: Hello! As an AI language model, I don't have feelings, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\n",
            "\n",
            "User: What is the capital of France?\n",
            "Assistant: The capital of France is Paris.\n",
            "\n",
            "User: Can you tell me a joke?\n",
            "Assistant: Sure, here's a joke for you:\n",
            "\n",
            "Why did the scarecrow win an award? Because he was outstanding in his field!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import boto3\n",
        "import json\n",
        "\n",
        "# Set up the Amazon Bedrock client\n",
        "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
        "\n",
        "# Define the model ID\n",
        "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
        "\n",
        "# List of user messages to send one by one\n",
        "user_messages = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"What is the capital of France?\",\n",
        "    \"Can you tell me a joke?\"\n",
        "]\n",
        "\n",
        "# Initialize conversation history\n",
        "conversation = []\n",
        "\n",
        "# Iterate through user messages and get a response for each\n",
        "for message in user_messages:\n",
        "    # Add the user message to the conversation\n",
        "    conversation.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": message}]})\n",
        "\n",
        "    # Construct the payload for this request\n",
        "    payload = {\n",
        "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "        \"max_tokens\": 256,\n",
        "        \"temperature\": 0.9,\n",
        "        \"top_k\": 250,\n",
        "        \"top_p\": 1,\n",
        "        \"messages\": conversation  # Send the full conversation history\n",
        "    }\n",
        "\n",
        "    # Invoke the model\n",
        "    response = bedrock_client.invoke_model(\n",
        "        modelId=model_id,\n",
        "        body=json.dumps(payload)\n",
        "    )\n",
        "\n",
        "    # Process the response\n",
        "    result = json.loads(response[\"body\"].read())\n",
        "\n",
        "    # Extract the assistant's response\n",
        "    if \"content\" in result and isinstance(result[\"content\"], list):\n",
        "        assistant_response = result[\"content\"][0][\"text\"]\n",
        "        print(f\"User: {message}\")\n",
        "        print(f\"Assistant: {assistant_response}\\n\")\n",
        "\n",
        "        # Add the assistant response to the conversation history\n",
        "        conversation.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant_response}]})\n",
        "\n",
        "    else:\n",
        "        print(\"Error: No valid 'content' found in the response.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
